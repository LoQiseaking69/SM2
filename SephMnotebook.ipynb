{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoQiseaking69/SM2/blob/main/SephMnotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5wSOBgUfQWu"
      },
      "source": [
        "# Imports\n",
        "This section includes all necessary library imports. Ensure that all the libraries used in the notebook are imported here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update system packages and install dependencies\n",
        "!apt-get update -y\n",
        "!apt-get install -y python-dev swig python-pygame\n",
        "\n",
        "# Install gym and the box2d environment\n",
        "!pip install gym[box2d]\n"
      ],
      "metadata": {
        "id": "3anT8WG-WEzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4cji40FJfQW1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, regularizers\n",
        "from collections import deque\n",
        "import gym\n",
        "import random\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9wbze9DfQW4"
      },
      "source": [
        "# Model Build\n",
        "Detailed description of the model building process. This section should include information about the architecture, layers used, and any custom components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Gw5GerX5fQW4"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def store_transition(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return None\n",
        "        samples = np.array(random.sample(self.buffer, batch_size), dtype=object)\n",
        "        return [np.stack(samples[:, i]) for i in range(samples.shape[1])]\n",
        "\n",
        "class RBMLayer(layers.Layer):\n",
        "    def __init__(self, num_hidden_units):\n",
        "        super(RBMLayer, self).__init__()\n",
        "        self.num_hidden_units = num_hidden_units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if len(input_shape) != 2:\n",
        "            raise ValueError(\"RBMLayer expects input shape of length 2\")\n",
        "        self.rbm_weights = self.add_weight(shape=(input_shape[-1], self.num_hidden_units),\n",
        "                                           initializer='glorot_uniform',\n",
        "                                           trainable=True)\n",
        "        self.biases = self.add_weight(shape=(self.num_hidden_units,),\n",
        "                                      initializer='zeros',\n",
        "                                      trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        activation = tf.matmul(inputs, self.rbm_weights) + self.biases\n",
        "        return tf.nn.sigmoid(activation)\n",
        "\n",
        "class QLearningLayer(layers.Layer):\n",
        "    def __init__(self, action_space_size, learning_rate=0.001, gamma=0.99, epsilon=0.1):\n",
        "        super(QLearningLayer, self).__init__()\n",
        "        self.action_space_size = action_space_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.min_epsilon = 0.01\n",
        "        self.buffer_index = 0\n",
        "        self.replay_buffer = ReplayBuffer(100000)\n",
        "        self.q_network = models.Sequential([\n",
        "            layers.Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.01)),\n",
        "            layers.Dense(64, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.01)),  # Added another dense layer\n",
        "            layers.Dense(self.action_space_size, activation='tanh', kernel_initializer='glorot_uniform')  # For continuous actions\n",
        "        ])\n",
        "        self.q_network.compile(optimizer=optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "        self.target_q_network = models.clone_model(self.q_network)\n",
        "\n",
        "    def call(self, state):\n",
        "        return self.q_network(state)\n",
        "\n",
        "    def update(self, batch_size):\n",
        "        data = self.replay_buffer.sample_buffer(batch_size)\n",
        "        if data is None:\n",
        "            return\n",
        "        states, actions, rewards, next_states, dones = data\n",
        "        target_q_values = rewards + (1 - dones) * self.gamma * np.max(self.target_q_network.predict(next_states), axis=1)\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = tf.reduce_sum(self.q_network(states) * tf.one_hot(actions, self.action_space_size), axis=1)\n",
        "            loss = tf.reduce_mean(tf.square(target_q_values - q_values))\n",
        "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
        "        self.q_network.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
        "        self.buffer_index += 1\n",
        "        if self.buffer_index % 1000 == 0:\n",
        "            self.target_q_network.set_weights(self.q_network.get_weights())\n",
        "        if self.epsilon > self.min_epsilon:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.store_transition((state, action, reward, next_state, done))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "      if np.random.rand() < self.epsilon:\n",
        "        return np.random.randint(self.action_space_size)\n",
        "      else:\n",
        "        q_values = self.q_network.predict(state)\n",
        "        action = np.argmax(q_values[0])\n",
        "        return np.clip(action, 0, self.action_space_size - 1)\n",
        "\n",
        "\n",
        "def create_neural_network_model(input_dim, num_hidden_units, action_space_size):\n",
        "    input_layer = layers.Input(shape=(input_dim,))  # Adjusted for 24-dimensional input\n",
        "\n",
        "    # A simpler architecture\n",
        "    x = layers.Dense(128, activation='relu', kernel_initializer='he_uniform')(input_layer)\n",
        "    x = layers.Dense(64, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x_rbm = RBMLayer(num_hidden_units)(x)  # Including your RBM layer\n",
        "    q_learning_layer = QLearningLayer(action_space_size)(x_rbm)  # Q-Learning layer\n",
        "\n",
        "    model = models.Model(inputs=input_layer, outputs=q_learning_layer)\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "input_dim = 24  # BipedalWalker observation space dimension\n",
        "num_hidden_units = 128  # Example value\n",
        "action_space_size = 4  # BipedalWalker action space dimension\n",
        "\n",
        "model = create_neural_network_model(input_dim, num_hidden_units, action_space_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHd1BtyNfQW6"
      },
      "source": [
        "# Train\n",
        "This section is dedicated to the training process of the model. It should include details about the training loop, optimization process, and any data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "WviaWj-FfQW7",
        "outputId": "6851c665-eea9-4feb-cfd6-69d5773ceb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 161ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "invalid index to scalar variable.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-3e20214060f5>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mq_learning_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQLearningLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_space_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtrain_model_in_bipedalwalker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_learning_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-3e20214060f5>\u001b[0m in \u001b[0;36mtrain_model_in_bipedalwalker\u001b[0;34m(env_name, q_learning_layer, num_episodes, epsilon)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_learning_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use Q-learning layer to choose action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py\u001b[0m in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;34m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;31m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     assert isinstance(\n\u001b[1;32m    224\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPEED_KNEE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPEED_HIP\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m             self.joints[0].maxMotorTorque = float(\n\u001b[1;32m    535\u001b[0m                 \u001b[0mMOTORS_TORQUE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def train_model_in_bipedalwalker(env_name, q_learning_layer, num_episodes, epsilon=0.1):\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        state = np.array(state).reshape(1, -1)\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = q_learning_layer.choose_action(state)  # Use Q-learning layer to choose action\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = np.array(next_state).reshape(1, -1)\n",
        "\n",
        "            # Assuming store_transition and update are methods of the QLearningLayer\n",
        "            q_learning_layer.store_transition(state, action, reward, next_state, done)\n",
        "            q_learning_layer.update(batch_size=32)  # Update the Q-learning layer with a batch size of 32\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        logger.info(f'Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}')\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # Save the trained model\n",
        "    save_path = 'trained_model.h5'\n",
        "    q_learning_layer.save_weights(save_path)\n",
        "    logger.info(f\"Model saved successfully at {save_path}\")\n",
        "\n",
        "# Example usage\n",
        "env_name = 'BipedalWalker-v3'\n",
        "num_episodes = 1000\n",
        "epsilon = 0.1  # Define epsilon value for exploration\n",
        "\n",
        "q_learning_layer = QLearningLayer(action_space_size)\n",
        "train_model_in_bipedalwalker(env_name, q_learning_layer, num_episodes, epsilon=epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSJ0hF3XfQW-"
      },
      "source": [
        "# Eval\n",
        "Description of the evaluation process. This section should cover how the model is evaluated, including metrics used, test datasets, and interpretation of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cjyL5q2fQW_"
      },
      "outputs": [],
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define or import your custom layers here (RBMLayer, QLearningLayer)\n",
        "# Ensure these definitions are the same as in your model creation\n",
        "\n",
        "def load_model(model_path):\n",
        "    \"\"\"\n",
        "    Load the saved model from a specified path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        custom_objects = {'RBMLayer': RBMLayer, 'QLearningLayer': QLearningLayer}\n",
        "        model = models.load_model(model_path, custom_objects=custom_objects)\n",
        "        logger.info(\"Model loaded successfully.\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model: {e}\")\n",
        "        raise\n",
        "\n",
        "def evaluate_model(model, env_name, num_episodes):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the environment over a number of episodes.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    total_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        state = np.array(state).reshape(1, -1)  # Reshape for model input\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = model.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = np.array(next_state).reshape(1, -1)  # Reshape for model input\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        total_rewards.append(total_reward)\n",
        "        logger.info(f'Episode: {episode + 1}, Total Reward: {total_reward}')\n",
        "\n",
        "    env.close()\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    std_reward = np.std(total_rewards)\n",
        "    logger.info(f'Average Reward over {num_episodes} episodes: {avg_reward}')\n",
        "    logger.info(f'Standard Deviation of Reward: {std_reward}')\n",
        "    return avg_reward, std_reward\n",
        "\n",
        "# Example usage\n",
        "model_path = 'path_to_your_saved_model_directory/trained_model'  # Adjust this path as needed\n",
        "loaded_model = load_model(model_path)\n",
        "\n",
        "env_name = 'BipedalWalker-v3'\n",
        "num_episodes = 100  # Number of episodes for evaluation\n",
        "\n",
        "# Evaluate and print model performance\n",
        "avg_reward, std_reward = evaluate_model(loaded_model, env_name, num_episodes)\n",
        "print(f'Average Reward: {avg_reward}, Standard Deviation of Reward: {std_reward}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}