{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoQiseaking69/SM2/blob/main/SephMnotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5wSOBgUfQWu"
      },
      "source": [
        "# Imports\n",
        "This section includes all necessary library imports. Ensure that all the libraries used in the notebook are imported here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update system packages and install dependencies\n",
        "!apt-get update -y\n",
        "!apt-get install -y python-dev swig python-pygame\n",
        "\n",
        "# Install gym and the box2d environment\n",
        "!pip install gym[box2d]\n"
      ],
      "metadata": {
        "id": "3anT8WG-WEzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4cji40FJfQW1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, regularizers\n",
        "from collections import deque\n",
        "import gym\n",
        "import random\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9wbze9DfQW4"
      },
      "source": [
        "# Model Build\n",
        "Detailed description of the model building process. This section should include information about the architecture, layers used, and any custom components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Gw5GerX5fQW4"
      },
      "outputs": [],
      "source": [
        "# Prioritized Replay Buffer for storing transitions\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, max_size: int, alpha: float = 0.6):\n",
        "        if max_size <= 0:\n",
        "            raise ValueError(\"max_size must be greater than 0\")\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "        self.priorities = deque(maxlen=max_size)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def store_transition(self, transition: tuple):\n",
        "        if not isinstance(transition, tuple):\n",
        "            raise ValueError(\"Transition must be a tuple\")\n",
        "        max_priority = max(self.priorities, default=1.0)\n",
        "        self.buffer.append(transition)\n",
        "        self.priorities.append(max_priority)\n",
        "\n",
        "    def sample_buffer(self, batch_size: int, beta: float = 0.4):\n",
        "        if batch_size <= 0:\n",
        "            raise ValueError(\"batch_size must be greater than 0\")\n",
        "        if len(self.buffer) < batch_size:\n",
        "            logger.warning(\"Not enough elements in the buffer to sample\")\n",
        "            return None\n",
        "\n",
        "        priorities = np.array(self.priorities)\n",
        "        probabilities = priorities ** self.alpha\n",
        "        probabilities /= probabilities.sum()\n",
        "\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
        "        samples = [self.buffer[i] for i in indices]\n",
        "        samples = np.array(samples, dtype=object)\n",
        "\n",
        "        weights = (len(self.buffer) * probabilities[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "\n",
        "        return [np.stack(samples[:, i]) for i in range(samples.shape[1])], indices, weights\n",
        "\n",
        "    def update_priorities(self, batch_indices, batch_priorities):\n",
        "        for idx, priority in zip(batch_indices, batch_priorities):\n",
        "            self.priorities[idx] = priority\n",
        "\n",
        "# RBM Layer implementation\n",
        "class RBMLayer(layers.Layer):\n",
        "    def __init__(self, num_hidden_units: int):\n",
        "        super(RBMLayer, self).__init__()\n",
        "        if num_hidden_units <= 0:\n",
        "            raise ValueError(\"Number of hidden units must be positive\")\n",
        "        self.num_hidden_units = num_hidden_units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if len(input_shape) != 2:\n",
        "            raise ValueError(\"RBMLayer expects input shape of length 2\")\n",
        "        self.rbm_weights = self.add_weight(shape=(input_shape[-1], self.num_hidden_units),\n",
        "                                           initializer='glorot_uniform',\n",
        "                                           trainable=True)\n",
        "        self.biases = self.add_weight(shape=(self.num_hidden_units,),\n",
        "                                      initializer='zeros',\n",
        "                                      trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        activation = tf.matmul(inputs, self.rbm_weights) + self.biases\n",
        "        return tf.nn.sigmoid(activation)\n",
        "\n",
        "# Q-Learning Layer implementation\n",
        "class QLearningLayer(layers.Layer):\n",
        "    def __init__(self, action_space_size: int, learning_rate: float = 0.001, gamma: float = 0.99, epsilon: float = 0.1):\n",
        "        super(QLearningLayer, self).__init__()\n",
        "        if action_space_size <= 0:\n",
        "            raise ValueError(\"Action space size must be positive\")\n",
        "        self.action_space_size = action_space_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.min_epsilon = 0.01\n",
        "        self.buffer_index = 0\n",
        "        self.replay_buffer = PrioritizedReplayBuffer(100000)\n",
        "        self.q_network = self._build_network()\n",
        "        self.target_q_network = models.clone_model(self.q_network)\n",
        "        self.q_network.compile(optimizer=optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "\n",
        "    def _build_network(self) -> models.Sequential:\n",
        "        model = models.Sequential([\n",
        "            layers.Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.01)),\n",
        "            layers.Dense(64, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.01)),\n",
        "            layers.Dense(self.action_space_size, activation='linear', kernel_initializer='glorot_uniform')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def call(self, state: np.ndarray) -> tf.Tensor:\n",
        "        if not isinstance(state, np.ndarray):\n",
        "            raise ValueError(\"State must be a numpy array\")\n",
        "        return self.q_network(state)\n",
        "\n",
        "    def update(self, batch_size: int, beta: float = 0.4):\n",
        "        data = self.replay_buffer.sample_buffer(batch_size, beta)\n",
        "        if data is None:\n",
        "            return\n",
        "        states, actions, rewards, next_states, dones = data[0]\n",
        "        indices, weights = data[1], data[2]\n",
        "\n",
        "        target_q_values = rewards + (1 - dones) * self.gamma * np.max(self.target_q_network.predict(next_states), axis=1)\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = tf.reduce_sum(self.q_network(states) * tf.one_hot(actions, self.action_space_size), axis=1)\n",
        "            loss = tf.reduce_mean(weights * tf.square(target_q_values - q_values))\n",
        "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
        "        self.q_network.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
        "\n",
        "        self.buffer_index += 1\n",
        "        if self.buffer_index % 1000 == 0:\n",
        "            self.target_q_network.set_weights(self.q_network.get_weights())\n",
        "        if self.epsilon > self.min_epsilon:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        priorities = np.abs(target_q_values - q_values) + 1e-6\n",
        "        self.replay_buffer.update_priorities(indices, priorities)\n",
        "\n",
        "    def store_transition(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool):\n",
        "        if not isinstance(state, np.ndarray) or not isinstance(next_state, np.ndarray):\n",
        "            raise ValueError(\"State and next_state must be numpy arrays\")\n",
        "        self.replay_buffer.store_transition((state, action, reward, next_state, done))\n",
        "\n",
        "    def choose_action(self, state: np.ndarray) -> int:\n",
        "        if not isinstance(state, np.ndarray):\n",
        "            raise ValueError(\"State must be a numpy array\")\n",
        "        state = np.array(state).reshape(1, -1)\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.action_space_size)\n",
        "        q_values = self.q_network.predict(state)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def save_weights(self, filepath: str):\n",
        "        if not filepath.endswith('.h5'):\n",
        "            raise ValueError(\"File path must end with '.h5'\")\n",
        "        self.q_network.save_weights(filepath)\n",
        "\n",
        "    def load_weights(self, filepath: str):\n",
        "        if not os.path.exists(filepath):\n",
        "            raise FileNotFoundError(\"The specified file does not exist\")\n",
        "        self.q_network.load_weights(filepath)\n",
        "\n",
        "# Neural Network model creation function\n",
        "def create_neural_network_model(input_dim: int, num_hidden_units: int, action_space_size: int) -> models.Model:\n",
        "    if input_dim <= 0 or num_hidden_units <= 0 or action_space_size <= 0:\n",
        "        raise ValueError(\"Input dimensions and action space size must be positive\")\n",
        "    input_layer = layers.Input(shape=(input_dim,))\n",
        "    x = layers.Dense(128, activation='relu', kernel_initializer='he_uniform')(input_layer)\n",
        "    x = layers.Dense(64, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x_rbm = RBMLayer(num_hidden_units)(x)\n",
        "    q_learning_layer = QLearningLayer(action_space_size)(x_rbm)\n",
        "    model = models.Model(inputs=input_layer, outputs=q_learning_layer)\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "input_dim = 24  # BipedalWalker observation space dimension\n",
        "num_hidden_units = 128  # Example value\n",
        "action_space_size = 4  # BipedalWalker action space dimension\n",
        "\n",
        "model = create_neural_network_model(input_dim, num_hidden_units, action_space_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHd1BtyNfQW6"
      },
      "source": [
        "# Train\n",
        "This section is dedicated to the training process of the model. It should include details about the training loop, optimization process, and any data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "WviaWj-FfQW7"
      },
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def train_model_in_bipedalwalker(env_name, q_learning_layer, num_episodes, epsilon=0.1):\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        state = np.array(state).reshape(1, -1)\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = q_learning_layer.choose_action(state)  # Use Q-learning layer to choose action\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = np.array(next_state).reshape(1, -1)\n",
        "\n",
        "            # Assuming store_transition and update are methods of the QLearningLayer\n",
        "            q_learning_layer.store_transition(state, action, reward, next_state, done)\n",
        "            q_learning_layer.update(batch_size=32)  # Update the Q-learning layer with a batch size of 32\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        logger.info(f'Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}')\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # Save the trained model\n",
        "    save_path = 'trained_model.h5'\n",
        "    q_learning_layer.save_weights(save_path)\n",
        "    logger.info(f\"Model saved successfully at {save_path}\")\n",
        "\n",
        "# Example usage\n",
        "env_name = 'BipedalWalker-v3'\n",
        "num_episodes = 1000\n",
        "epsilon = 0.1  # Define epsilon value for exploration\n",
        "\n",
        "q_learning_layer = QLearningLayer(action_space_size)\n",
        "train_model_in_bipedalwalker(env_name, q_learning_layer, num_episodes, epsilon=epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSJ0hF3XfQW-"
      },
      "source": [
        "# Eval\n",
        "Description of the evaluation process. This section should cover how the model is evaluated, including metrics used, test datasets, and interpretation of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cjyL5q2fQW_"
      },
      "outputs": [],
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define or import your custom layers here (RBMLayer, QLearningLayer)\n",
        "# Ensure these definitions are the same as in your model creation\n",
        "\n",
        "def load_model(model_path):\n",
        "    \"\"\"\n",
        "    Load the saved model from a specified path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        custom_objects = {'RBMLayer': RBMLayer, 'QLearningLayer': QLearningLayer}\n",
        "        model = models.load_model(model_path, custom_objects=custom_objects)\n",
        "        logger.info(\"Model loaded successfully.\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model: {e}\")\n",
        "        raise\n",
        "\n",
        "def evaluate_model(model, env_name, num_episodes):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the environment over a number of episodes.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    total_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        state = np.array(state).reshape(1, -1)  # Reshape for model input\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = model.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = np.array(next_state).reshape(1, -1)  # Reshape for model input\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        total_rewards.append(total_reward)\n",
        "        logger.info(f'Episode: {episode + 1}, Total Reward: {total_reward}')\n",
        "\n",
        "    env.close()\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    std_reward = np.std(total_rewards)\n",
        "    logger.info(f'Average Reward over {num_episodes} episodes: {avg_reward}')\n",
        "    logger.info(f'Standard Deviation of Reward: {std_reward}')\n",
        "    return avg_reward, std_reward\n",
        "\n",
        "# Example usage\n",
        "model_path = 'trained_model.h5'\n",
        "loaded_model = load_model(model_path)\n",
        "\n",
        "env_name = 'BipedalWalker-v3'\n",
        "num_episodes = 100  # Number of episodes for evaluation\n",
        "\n",
        "# Evaluate and print model performance\n",
        "avg_reward, std_reward = evaluate_model(loaded_model, env_name, num_episodes)\n",
        "print(f'Average Reward: {avg_reward}, Standard Deviation of Reward: {std_reward}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
