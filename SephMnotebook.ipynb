{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoQiseaking69/SM2/blob/main/SephMnotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5wSOBgUfQWu"
      },
      "source": [
        "# Imports\n",
        "This section includes all necessary library imports. Ensure that all the libraries used in the notebook are imported here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cji40FJfQW1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, regularizers\n",
        "from collections import deque\n",
        "import gym\n",
        "import random\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9wbze9DfQW4"
      },
      "source": [
        "# Model Build\n",
        "Detailed description of the model building process. This section should include information about the architecture, layers used, and any custom components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw5GerX5fQW4"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Replay Buffer for storing transitions.\"\"\"\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def store_transition(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return None\n",
        "        samples = np.array(random.sample(self.buffer, batch_size), dtype=object)\n",
        "        return [np.stack(samples[:, i]) for i in range(samples.shape[1])]\n",
        "\n",
        "class RBMLayer(layers.Layer):\n",
        "    \"\"\"Restricted Boltzmann Machine Layer.\"\"\"\n",
        "    def __init__(self, num_hidden_units):\n",
        "        super(RBMLayer, self).__init__()\n",
        "        self.num_hidden_units = num_hidden_units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if len(input_shape) != 2:\n",
        "            raise ValueError(\"RBMLayer expects input shape of length 2\")\n",
        "        self.rbm_weights = self.add_weight(shape=(input_shape[-1], self.num_hidden_units),\n",
        "                                           initializer='glorot_uniform',\n",
        "                                           trainable=True)\n",
        "        self.biases = self.add_weight(shape=(self.num_hidden_units,),\n",
        "                                      initializer='zeros',\n",
        "                                      trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        activation = tf.matmul(inputs, self.rbm_weights) + self.biases\n",
        "        return tf.nn.sigmoid(activation)\n",
        "\n",
        "class QLearningLayer(layers.Layer):\n",
        "    \"\"\"Q-Learning Layer for reinforcement learning.\"\"\"\n",
        "    def __init__(self, action_space_size, learning_rate=0.001, gamma=0.99, epsilon=0.1):\n",
        "        super(QLearningLayer, self).__init__()\n",
        "        self.action_space_size = action_space_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.min_epsilon = 0.01\n",
        "        self.replay_buffer = ReplayBuffer(100000)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.q_network = models.Sequential([\n",
        "            layers.Dense(256, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.01)),\n",
        "            layers.Dense(self.action_space_size, kernel_initializer='glorot_uniform')\n",
        "        ])\n",
        "        self.q_network.compile(optimizer=optimizers.Adam(learning_rate=self.learning_rate), loss='mse')\n",
        "        self.target_q_network = models.clone_model(self.q_network)\n",
        "\n",
        "    def call(self, state):\n",
        "        return self.q_network(state)\n",
        "\n",
        "    def update(self, batch_size):\n",
        "        data = self.replay_buffer.sample_buffer(batch_size)\n",
        "        if data is None:\n",
        "            return\n",
        "        states, actions, rewards, next_states, dones = data\n",
        "        target_q_values = rewards + (1 - dones) * self.gamma * np.max(self.target_q_network.predict(next_states), axis=1)\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = tf.reduce_sum(self.q_network(states) * tf.one_hot(actions, self.action_space_size), axis=1)\n",
        "            loss = tf.reduce_mean(tf.square(target_q_values - q_values))\n",
        "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
        "        self.q_network.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
        "        if self.epsilon > self.min_epsilon:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        if self.buffer_index % 1000 == 0:\n",
        "            self.target_q_network.set_weights(self.q_network.get_weights())\n",
        "\n",
        "    def store_transition(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.store_transition((state, action, reward, next_state, done))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.action_space_size)\n",
        "        else:\n",
        "            q_values = self.q_network.predict(state[np.newaxis, :])\n",
        "            return np.argmax(q_values[0])\n",
        "\n",
        "def positional_encoding(seq_length, d_model):\n",
        "    \"\"\"Positional encoding for sequence data.\"\"\"\n",
        "    position = np.arange(seq_length)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    pos_encoding = np.zeros((seq_length, d_model))\n",
        "    pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
        "    pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
        "    return tf.constant(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    \"\"\"Transformer encoder layer.\"\"\"\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Dense(ff_dim, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(inputs.shape[-1], kernel_initializer='glorot_uniform')(x)\n",
        "    return x + res\n",
        "\n",
        "def create_neural_network_model(seq_length, d_model, num_hidden_units, action_space_size):\n",
        "    \"\"\"Create a neural network model integrating various layers.\"\"\"\n",
        "    input_layer = layers.Input(shape=(seq_length, d_model))\n",
        "    x = positional_encoding(seq_length, d_model)\n",
        "    x = x + input_layer\n",
        "    x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=256)\n",
        "    x_lstm = layers.Bidirectional(layers.LSTM(128, return_sequences=True, kernel_initializer='glorot_uniform'))(x)\n",
        "    x_conv = layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x_lstm)\n",
        "\n",
        "    # Reshaping the output from Conv1D to match the 2D input expectation of RBMLayer\n",
        "    x_flatten = layers.Flatten()(x_conv)\n",
        "    x_rbm = RBMLayer(num_hidden_units)(x_flatten)\n",
        "\n",
        "    q_learning_layer = QLearningLayer(action_space_size)(x_rbm)\n",
        "\n",
        "    model = models.Model(inputs=input_layer, outputs=q_learning_layer)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHd1BtyNfQW6"
      },
      "source": [
        "# Train\n",
        "This section is dedicated to the training process of the model. It should include details about the training loop, optimization process, and any data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WviaWj-FfQW7",
        "outputId": "e40faef0-0af8-4dc6-8c72-28acfb160cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error creating environment BipedalWalker-v3: box2D is not installed, run `pip install gym[box2d]`\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model with appropriate parameters\n",
        "seq_length = 24  # Example value, set it according to your environment\n",
        "d_model = 16     # Example value, set it according to your environment\n",
        "num_hidden_units = 50  # Example value\n",
        "action_space_size = 4  # Example value, set it according to the environment\n",
        "model = create_neural_network_model(seq_length, d_model, num_hidden_units, action_space_size)\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Training Function\n",
        "def train_model_in_bipedalwalker(env_name, model, num_episodes, batch_size=64):\n",
        "    try:\n",
        "        env = gym.make(env_name)\n",
        "    except gym.error.Error as e:\n",
        "        logger.error(f\"Error creating environment {env_name}: {e}\")\n",
        "        return\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        try:\n",
        "            state = env.reset()\n",
        "            state = np.array(state).reshape(1, -1)  # Reshape for model input\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "\n",
        "            while not done:\n",
        "                action = model.choose_action(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                next_state = np.array(next_state).reshape(1, -1)  # Reshape for model input\n",
        "                model.store_transition(state, action, reward, next_state, done)\n",
        "                model.update(batch_size)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "            logger.info(f'Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}')\n",
        "        except Exception as e:\n",
        "            logger.error(f\"An error occurred in episode {episode + 1}: {e}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # Save the trained model in the current notebook directory\n",
        "    notebook_dir = os.getcwd()\n",
        "    save_path = os.path.join(notebook_dir, 'trained_model')\n",
        "    try:\n",
        "        model.save(save_path, save_format='tf')\n",
        "        logger.info(f\"Model saved successfully at {save_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred while saving the model: {e}\")\n",
        "\n",
        "# Example usage\n",
        "env_name = 'BipedalWalker-v3'\n",
        "num_episodes = 1000\n",
        "train_model_in_bipedalwalker(env_name, model, num_episodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSJ0hF3XfQW-"
      },
      "source": [
        "# Eval\n",
        "Description of the evaluation process. This section should cover how the model is evaluated, including metrics used, test datasets, and interpretation of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cjyL5q2fQW_"
      },
      "outputs": [],
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define or import your custom layers here (RBMLayer, QLearningLayer)\n",
        "# Ensure these definitions are the same as in your model creation\n",
        "\n",
        "def load_model(model_path):\n",
        "    \"\"\"\n",
        "    Load the saved model from a specified path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        custom_objects = {'RBMLayer': RBMLayer, 'QLearningLayer': QLearningLayer}\n",
        "        model = models.load_model(model_path, custom_objects=custom_objects)\n",
        "        logger.info(\"Model loaded successfully.\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model: {e}\")\n",
        "        raise\n",
        "\n",
        "def evaluate_model(model, env_name, num_episodes):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the environment over a number of episodes.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    total_rewards = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        state = np.array(state).reshape(1, -1)  # Reshape for model input\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = model.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = np.array(next_state).reshape(1, -1)  # Reshape for model input\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        total_rewards.append(total_reward)\n",
        "        logger.info(f'Episode: {episode + 1}, Total Reward: {total_reward}')\n",
        "\n",
        "    env.close()\n",
        "    avg_reward = np.mean(total_rewards)\n",
        "    std_reward = np.std(total_rewards)\n",
        "    logger.info(f'Average Reward over {num_episodes} episodes: {avg_reward}')\n",
        "    logger.info(f'Standard Deviation of Reward: {std_reward}')\n",
        "    return avg_reward, std_reward\n",
        "\n",
        "# Example usage\n",
        "model_path = 'path_to_your_saved_model_directory/trained_model'  # Adjust this path as needed\n",
        "loaded_model = load_model(model_path)\n",
        "\n",
        "env_name = 'BipedalWalker-v3'\n",
        "num_episodes = 100  # Number of episodes for evaluation\n",
        "\n",
        "# Evaluate and print model performance\n",
        "avg_reward, std_reward = evaluate_model(loaded_model, env_name, num_episodes)\n",
        "print(f'Average Reward: {avg_reward}, Standard Deviation of Reward: {std_reward}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}